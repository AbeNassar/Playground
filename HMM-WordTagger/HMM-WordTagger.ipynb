{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models For Word Tagging\n",
    "My attempt to build a trigrame model by extending the bigram model built by Katrin Erk which is found here:\n",
    "\n",
    "http://www.katrinerk.com/courses/python-worksheets/hidden-markov-models-for-pos-tagging-in-python\n",
    "\n",
    "## Introduction\n",
    "When we speak, because we took english in school, we know that each word we say has a part of speech associated with that word. For example if we consider the sentence \"Abraham ran swiftly\" we know that \"Abraham\" is a noun, \"ran\" is a verb and \"swiftly\" is an adjective. Transfering this knowledge to a computer to do natural language processing can be tricky since we have words that can have an ambiguous part of speech. One example is the word \"lead.\"  It can mean the metal, in which case it is a noun, but it can also be a verb as as with the sentence \"I will lead us to victory!\" So we need context to clarify this potential ambiguity. One way we can incoprperate context is by using a Markov chain which can models sequential processes. The only problem left is that the computer never actually \"sees\" the tags. When we write sentences we never include where the word is a noun or verb, etc. The Computer only sees the words. So to model this, we extend the idea of a Markov chain to a _hidden Markov chain_. The idea is that we know there is a Markov process that is hidden, in this case the word tags, but the information we do see, i.e. the words themselves, give us enough information to make inferences about the underlying hidden process.\n",
    "\n",
    "To be more specific, a Markov Chain is a process in which the probability of moving to the next state is only dependent on the current state and perhaps a few of the previous states, but the distant past. So, for a sequence $t_0, \\ldots, t_{k-3}, t_{k-2}, t_{k-1}, t_{k}$ The probability of $t_k$ given the entire history is given by\n",
    "\n",
    "\\begin{align*}\n",
    "  P( t_{k+1} | t_0, \\ldots, t_{k-3}, t_{k-2}, t_{k-1}, t_{k}) \n",
    "   &= P( t_{k+1} | t_{k}) &&\\text{First Order Markov Chain}\\\\\n",
    "  P( t_{k+1} | t_0, \\ldots, t_{k-3}, t_{k-2}, t_{k-1}, t_{k}) \n",
    "   &= P( t_{k+1} |  t_{k-1}, t_{k}) &&\\text{Second Order Markov Chain}\\\\\n",
    "  P( t_{k+1} | t_0, \\ldots, t_{k-3}, t_{k-2}, t_{k-1}, t_{k}) \n",
    "   &= P( t_{k+1} |  t_{k-2}, t_{k-1}, t_{k}) &&\\text{Third Order Markov Chain}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In other words, a first order Markov chain doesn't need any past information except for the last state to calculate the probability of the next state. A second order Markov chain only needs the last two states to calculate the probability, and so on. \n",
    "\n",
    "In this example, we will be modeling the tags with a second order Markov chain. The $\\{t_k\\}_{k\\in\\mathbb{N}}$ sequence will be our tags since we know that if we start a sentence with a determiner followed by a noun, we know there it is very likely that a verb will come next. At the same time the distant past is irrelevant because the next tag in a sentence will not depend on the any tags that apeared in the previous paragraph. We can model this pictorally as shown below. \n",
    "<img src=\"TagMC.png\" style=\"width:500px\">\n",
    "The function $P(T_{k}|T_{k-2}, T_{k-1})$ is the probability of transitioning to tag $T_k$ given that the previous two tags were $T_{k-2}$ and $T_{k-1}$.\n",
    "\n",
    "But again, the computer never sees the tags, it only sees the words. We can include the words in the diagram as shown below.\n",
    "<img src=\"HMM.png\" style=\"width:500px\">\n",
    "And the function $e(W|T)$ is the _emission_ probability of seeign word $W$ given that the current tag is $T$. With this model, we can calculate the joint probability of all words and tags as:\n",
    "\n",
    "$$   \n",
    "  P(w_1, \\dots, w_K, t_1, \\dots, t_K) \n",
    "   = P(t_1)\\cdot P(t_2|t_1)\\cdot \\prod_{i=3}^{K}P(t_{i}|t_{i-2}, t_{i-1})\\cdot \\prod_{j=1}^{K}P(w_{j}|t_{j})\n",
    "$$\n",
    "\n",
    "With the joint probability distribution in hand, it is easy to find conditional distributions. We specifically want $P(t_1, \\dots, t_K|w_1, \\dots, w_K)$ because we can choose the best tag sequence $\\{t_k^*\\}_{k=1}^{K}$ to be the sequnce that maximizes the conditioninal distribution probability. \n",
    "\n",
    "\\begin{align*}\n",
    "  \\{t_k^*\\}_{k=1}^{K} = \\mathop{\\text{argmax}}_{t_1,\\dots,t_K} P(t_1, \\dots, t_K|w_1, \\dots, w_K)\n",
    "\\end{align*}\n",
    "\n",
    "We will use the following lbraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brown corpus contain about a million tagged words you can see descriptioins of the tags using `nltk.help.brown_tagset()`. To easy make things a bit easier, I'll convert all words to lowercase. This mean that \"Cow\" and \"cow\" are treated as the same word instead of different words which results in a smaller emission matrix. I'll also only use the base form of the tag, which means that `DO+PPSS` and `DO*` are both converted to `DO`. This has the effect if decreasing the transition matrix and the emission matrix. First let me define a function to pull the base tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBaseTag(tagString):\n",
    "    \"\"\"Gets a simple version of an input tag. \n",
    "    \n",
    "    Example:\n",
    "    >>> getTag(\"WDT+BER+PP\")\n",
    "    [1] 'WDT'\n",
    "    >>> getTag(\"--\")\n",
    "    [2] '--'\n",
    "      \n",
    "    Input:\n",
    "      tagString (str): The full tag\n",
    "\n",
    "    Output:\n",
    "      str: if `tagString` is a word tag, it returns the base of the tag\n",
    "        if `tagString` is a punctuation tag, it returns `tagString`\n",
    "    \"\"\"\n",
    "    reTag = re.search(\"[A-Z]+\", tagString, flags=0)\n",
    "    if reTag:\n",
    "        return reTag.group(0)\n",
    "    else:\n",
    "        return tagString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_tags_words = [ ]\n",
    "for sent in brown.tagged_sents():\n",
    "    # sent is a list of word/tag pairs\n",
    "    # add START/START at the beginning\n",
    "    brown_tags_words.append( (\"START\", \"START\") )\n",
    "    brown_tags_words.append( (\"START\", \"START\") )\n",
    "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
    "    # shorten tags to 2 characters each\n",
    "    brown_tags_words.extend([ (getBaseTag(tag), word) for (word, tag) in sent ])\n",
    "    # then END/END\n",
    "    brown_tags_words.append( (\"END\", \"END\") )    \n",
    "    brown_tags_words.append( (\"END\", \"END\") )\n",
    "    \n",
    "# Placing data into data frame for convinience\n",
    "brown_df = pd.DataFrame({\"words\":list(map(lambda x: x[1], brown_tags_words)),\n",
    "                         \"tags\" :list(map(lambda x: x[0], brown_tags_words))})\n",
    "\n",
    "del brown_tags_words, brown # no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this points, I put all of the words and tags together in one data frame as opposed to the way it was before, which was split up into sentences. So now the data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>START</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP</td>\n",
       "      <td>Fulton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Grand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN</td>\n",
       "      <td>Jury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VBD</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NR</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AT</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tags   words\n",
       "0  START   START\n",
       "1  START   START\n",
       "2     AT     The\n",
       "3     NP  Fulton\n",
       "4     NN  County\n",
       "5     JJ   Grand\n",
       "6     NN    Jury\n",
       "7    VBD    said\n",
       "8     NR  Friday\n",
       "9     AT      an"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since I want to build a trigram model, that means I need to look a the previous two states. To acomplish this, I will pair up the words into pairs sperarated by the character `->`. The inital state will have the variable `initalState` which will be the previous two tags and the current state will have the variable name of `currentState` and will represent the tag of the previous word followed by the tag for the current word. \n",
    "\n",
    "To accomplish this, I will join the table with itself, but with the index decremented. And I'lld to this twice so that I have the three most recent tags. I will also add a column called `count` to count the occurances of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_df = brown_df[[\"tags\"]].join(brown_df[[\"tags\"]].shift(-1), lsuffix=\"_2\") \\\n",
    "                .join(brown_df[[\"tags\",\"words\"]].shift(-2), lsuffix=\"_1\", rsuffix=\"_0\") \\\n",
    "                .dropna()\n",
    "\n",
    "# This will be used to count occurances of each state \n",
    "trigram_df[\"counts\"] = np.ones(len(trigram_df))\n",
    "\n",
    "# join two concurent tags that define the states.\n",
    "trigram_df[\"initialState\"] = trigram_df.tags_2 + \" -> \" + trigram_df.tags_1\n",
    "trigram_df.drop('tags_2', axis=1, inplace=True)\n",
    "trigram_df[\"currentState\"] = trigram_df.tags_1 + \" -> \" + trigram_df.tags_0\n",
    "trigram_df.drop(['tags_1'], axis=1, inplace=True)\n",
    "trigram_df.rename(columns={\"tags_0\":\"currentTag\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currentTag</th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "      <th>initialState</th>\n",
       "      <th>currentState</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT</td>\n",
       "      <td>The</td>\n",
       "      <td>1.0</td>\n",
       "      <td>START -&gt; START</td>\n",
       "      <td>START -&gt; AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP</td>\n",
       "      <td>Fulton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>START -&gt; AT</td>\n",
       "      <td>AT -&gt; NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN</td>\n",
       "      <td>County</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AT -&gt; NP</td>\n",
       "      <td>NP -&gt; NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Grand</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NP -&gt; NN</td>\n",
       "      <td>NN -&gt; JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>Jury</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN -&gt; JJ</td>\n",
       "      <td>JJ -&gt; NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  currentTag   words  counts    initialState currentState\n",
       "0         AT     The     1.0  START -> START  START -> AT\n",
       "1         NP  Fulton     1.0     START -> AT     AT -> NP\n",
       "2         NN  County     1.0        AT -> NP     NP -> NN\n",
       "3         JJ   Grand     1.0        NP -> NN     NN -> JJ\n",
       "4         NN    Jury     1.0        NN -> JJ     JJ -> NN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating The Transistion Matrix\n",
    "\n",
    "With the states defined, I need to count the number of transitions from `initialSate` to `currentState` followed by normalizing by dividing by the sum so each row represents a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transitionDF = trigram_df[[\"initialState\", \"currentState\", \"counts\"]] \\\n",
    "    .pivot_table(columns=\"currentState\", \n",
    "                       index=\"initialState\", \n",
    "                       values=\"counts\", \n",
    "                       fill_value=0,\n",
    "                       aggfunc=np.sum) \\\n",
    "    .apply(lambda row: row/row.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the transition matrix in the form of a dataframe. Which is a large sparce matrix that is too ugly to show, but if I want to find what percentage of times the sequence `NN`, `VBD` was preceded by `NR`, I just look for the initial state of `'NN -> VBD'` and current state of `'VBD -> NR'` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0045772751750134625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitionDF[\"VBD -> NR\"][\"NN -> VBD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that from the state `'NN-TL -> VBD'`, the next state was `'VBD -> NR'` about 0.46% of the time. Mathematically, this is the same as ${\\mathbb{P}(t_k={\\tt NR} \\,|\\, t_{k-2}={\\tt NN}, t_{k-1}={\\tt VBD})}$\n",
    "\n",
    "## Calculating The Emission Matrix\n",
    "The emission matrix will the contain the probabilities of having a certain work given the current state, which recall is the previous tag followed by the current tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emissionDF = trigram_df[[\"words\", \"currentTag\", \"counts\"]] \\\n",
    "    .pivot_table(columns=\"words\", \n",
    "                       index=\"currentTag\", \n",
    "                       values=\"counts\", \n",
    "                       fill_value=0,\n",
    "                       aggfunc=np.sum) \\\n",
    "    .apply(lambda row: row/row.sum(), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63294205516921187"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissionDF[\"the\"][\"AT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that given that the current tag is in the state `\"AT\"`, the probability of getting the word `\"the\"` is about 63%. This is equivalent to $\\mathbb{P}(w_k={\\tt The} \\;|\\; t_k = {\\tt AT})$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
